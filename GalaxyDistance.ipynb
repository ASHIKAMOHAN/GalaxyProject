{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load datasets\n",
    "def load_data():\n",
    "    happy_files = ['Happy/happy_A.txt', 'Happy/happy_B.txt', 'Happy/happy_C.txt', 'Happy/happy_D.txt']\n",
    "    teddy_files = ['Teddy/teddy_A.txt', 'Teddy/teddy_B.txt', 'Teddy/teddy_C.txt', 'Teddy/teddy_D.txt']\n",
    "\n",
    "    happy_dfs = [pd.read_csv(file, delimiter='\\t') for file in happy_files]\n",
    "    teddy_dfs = [pd.read_csv(file, delimiter='\\t', skiprows=6) for file in teddy_files]\n",
    "\n",
    "    return happy_dfs, teddy_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "happy_dfs, teddy_dfs = load_data()\n",
    "\n",
    "# Define the correct column names\n",
    "column_names = ['id', 'mag_r', 'u-g', 'g-r', 'r-i', 'i-z', 'z_spec', 'feat1', 'feat2', 'feat3', 'feat4', 'feat5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and rename columns\n",
    "def clean_and_rename_columns(dfs, column_names):\n",
    "    cleaned_dfs = []\n",
    "    for df in dfs:\n",
    "        if df.shape[1] == 1:\n",
    "            df = df.iloc[:, 0].str.split(expand=True)\n",
    "        df.columns = column_names\n",
    "        cleaned_dfs.append(df)\n",
    "    return cleaned_dfs\n",
    "\n",
    "# Apply the function to all datasets\n",
    "happy_dfs = clean_and_rename_columns(happy_dfs, column_names)\n",
    "teddy_dfs = clean_and_rename_columns(teddy_dfs, column_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "def preprocess_data(df):\n",
    "    X = df.drop('z_spec', axis=1).astype(float)\n",
    "    y = df['z_spec'].astype(float)\n",
    "    return X, y\n",
    "\n",
    "# Preprocess the datasets\n",
    "# Happy datasets\n",
    "X_train_happy, y_train_happy = preprocess_data(happy_dfs[0])  # Happy A: Training/Calibration set\n",
    "X_val_happy, y_val_happy = preprocess_data(happy_dfs[1])      # Happy B: Validation set\n",
    "X_test_happy, y_test_happy = preprocess_data(happy_dfs[2])    # Happy C: Validation set\n",
    "# Happy D is not used\n",
    "\n",
    "# Teddy datasets\n",
    "X_train_teddy, y_train_teddy = preprocess_data(teddy_dfs[0])  # Teddy A: Training/Calibration set\n",
    "X_val_teddy, y_val_teddy = preprocess_data(teddy_dfs[1])      # Teddy B: Validation set\n",
    "X_test_teddy, y_test_teddy = preprocess_data(teddy_dfs[2])    # Teddy C: Validation set\n",
    "# Teddy D is not used\n",
    "\n",
    "# Combine Happy and Teddy datasets for a comprehensive training set\n",
    "X_train = pd.concat([X_train_happy, X_train_teddy])\n",
    "y_train = pd.concat([y_train_happy, y_train_teddy])\n",
    "\n",
    "X_val = pd.concat([X_val_happy, X_val_teddy])\n",
    "y_val = pd.concat([y_val_happy, y_val_teddy])\n",
    "\n",
    "X_test = pd.concat([X_test_happy, X_test_teddy])\n",
    "y_test = pd.concat([y_test_happy, y_test_teddy])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weights\n",
    "def load_weights():\n",
    "    weight_files = [\n",
    "        'Happy/weights_for_B.txt', 'Happy/weights_for_C.txt', 'Happy/weights_for_D.txt',\n",
    "        'Teddy/weights_for_B.txt', 'Teddy/weights_for_C.txt', 'Teddy/weights_for_D.txt'\n",
    "    ]\n",
    "    weights = [pd.read_csv(file, delimiter='\\t', header=None).values.flatten() for file in weight_files]\n",
    "    return weights\n",
    "\n",
    "weights = load_weights()\n",
    "# Update weights_val and weights_test lengths\n",
    "weights_val = np.concatenate([weights[0], weights[3][:len(X_val_teddy)]])  # Combine Happy B and truncate Teddy B weights for validation\n",
    "weights_test = np.concatenate([weights[1], weights[4][:len(X_test_teddy)]]) # Combine Happy C and truncate Teddy C weights for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X_train: 149259\n",
      "Length of y_train: 149259\n",
      "Length of X_val: 149457\n",
      "Length of y_val: 149457\n",
      "Length of weights_val: 149259\n",
      "Length of X_test: 158295\n",
      "Length of y_test: 158295\n",
      "Length of weights_test: 149259\n"
     ]
    }
   ],
   "source": [
    "# Check for consistency in data lengths\n",
    "print(\"Length of X_train:\", len(X_train))\n",
    "print(\"Length of y_train:\", len(y_train))\n",
    "print(\"Length of X_val:\", len(X_val))\n",
    "print(\"Length of y_val:\", len(y_val))\n",
    "print(\"Length of weights_val:\", len(weights_val))\n",
    "print(\"Length of X_test:\", len(X_test))\n",
    "print(\"Length of y_test:\", len(y_test))\n",
    "print(\"Length of weights_test:\", len(weights_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training functions\n",
    "\n",
    "# Random Forest\n",
    "def train_random_forest(X_train, y_train, X_val, y_val, weights_train, weights_val):\n",
    "    rf = RandomForestRegressor()\n",
    "    rf.fit(X_train, y_train, sample_weight=weights_train)\n",
    "    y_pred_rf = rf.predict(X_val)\n",
    "    rmse_rf = mean_squared_error(y_val, y_pred_rf, squared=False, sample_weight=weights_val)\n",
    "    return y_pred_rf, rmse_rf\n",
    "\n",
    "# Gradient Boosting\n",
    "def train_gradient_boost(X_train, y_train, X_val, y_val, weights_train, weights_val):\n",
    "    gb = GradientBoostingRegressor()\n",
    "    gb.fit(X_train, y_train, sample_weight=weights_train)\n",
    "    y_pred_gb = gb.predict(X_val)\n",
    "    rmse_gb = mean_squared_error(y_val, y_pred_gb, squared=False, sample_weight=weights_val)\n",
    "    return y_pred_gb, rmse_gb\n",
    "\n",
    "# Gaussian Processes\n",
    "def train_gaussian_process(X_train, y_train, X_val, y_val, weights_train, weights_val):\n",
    "    gp = GaussianProcessRegressor()\n",
    "    gp.fit(X_train, y_train)\n",
    "    y_pred_gp = gp.predict(X_val)\n",
    "    rmse_gp = mean_squared_error(y_val, y_pred_gp, squared=False, sample_weight=weights_val)\n",
    "    return y_pred_gp, rmse_gp\n",
    "\n",
    "# Neural Network\n",
    "def train_neural_network(X_train, y_train, X_val, y_val, weights_train, weights_val):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(X_train, y_train, sample_weight=weights_train, epochs=100, validation_data=(X_val, y_val, weights_val), verbose=0)\n",
    "    \n",
    "    y_pred_nn = model.predict(X_val).flatten()\n",
    "    rmse_nn = mean_squared_error(y_val, y_pred_nn, squared=False, sample_weight=weights_val)\n",
    "    return y_pred_nn, rmse_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [149457, 149457, 149259]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y_pred, rmse\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Train and evaluate models\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m y_pred_rf, rmse_rf \u001b[38;5;241m=\u001b[39m train_random_forest(X_train, y_train, X_val, y_val, np\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;28mlen\u001b[39m(X_train)), weights_val)\n\u001b[1;32m     21\u001b[0m y_pred_gb, rmse_gb \u001b[38;5;241m=\u001b[39m train_gradient_boost(X_train, y_train, X_val, y_val, np\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;28mlen\u001b[39m(X_train)), weights_val)\n\u001b[1;32m     22\u001b[0m y_pred_gp, rmse_gp \u001b[38;5;241m=\u001b[39m train_gaussian_process(X_train, y_train, X_val, y_val, np\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;28mlen\u001b[39m(X_train)), weights_val)\n",
      "Cell \u001b[0;32mIn[44], line 8\u001b[0m, in \u001b[0;36mtrain_random_forest\u001b[0;34m(X_train, y_train, X_val, y_val, weights_train, weights_val)\u001b[0m\n\u001b[1;32m      6\u001b[0m rf\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, sample_weight\u001b[38;5;241m=\u001b[39mweights_train)\n\u001b[1;32m      7\u001b[0m y_pred_rf \u001b[38;5;241m=\u001b[39m rf\u001b[38;5;241m.\u001b[39mpredict(X_val)\n\u001b[0;32m----> 8\u001b[0m rmse_rf \u001b[38;5;241m=\u001b[39m mean_squared_error(y_val, y_pred_rf, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39mweights_val)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y_pred_rf, rmse_rf\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    209\u001b[0m         )\n\u001b[1;32m    210\u001b[0m     ):\n\u001b[0;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_regression.py:477\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Mean squared error regression loss.\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \n\u001b[1;32m    419\u001b[0m \u001b[38;5;124;03mRead more in the :ref:`User Guide <mean_squared_error>`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m0.825...\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    474\u001b[0m y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m _check_reg_targets(\n\u001b[1;32m    475\u001b[0m     y_true, y_pred, multioutput\n\u001b[1;32m    476\u001b[0m )\n\u001b[0;32m--> 477\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    478\u001b[0m output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m squared:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py:409\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    407\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 409\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[1;32m    412\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [149457, 149457, 149259]"
     ]
    }
   ],
   "source": [
    "# Custom Model 1\n",
    "def train_custom_model_1(X_train, y_train, X_val, y_val, weights_train, weights_val):\n",
    "    y_pred = np.zeros(X_val.shape[0])\n",
    "    rmse = mean_squared_error(y_val, y_pred, squared=False, sample_weight=weights_val)\n",
    "    return y_pred, rmse\n",
    "\n",
    "# Custom Model 2\n",
    "def train_custom_model_2(X_train, y_train, X_val, y_val, weights_train, weights_val):\n",
    "    y_pred = np.zeros(X_val.shape[0])\n",
    "    rmse = mean_squared_error(y_val, y_pred, squared=False, sample_weight=weights_val)\n",
    "    return y_pred, rmse\n",
    "\n",
    "# Template Fitting Model\n",
    "def train_template_fitting(X_train, y_train, X_val, y_val, weights_train, weights_val):\n",
    "    y_pred = np.zeros(X_val.shape[0])\n",
    "    rmse = mean_squared_error(y_val, y_pred, squared=False, sample_weight=weights_val)\n",
    "    return y_pred, rmse\n",
    "\n",
    "# Train and evaluate models\n",
    "y_pred_rf, rmse_rf = train_random_forest(X_train, y_train, X_val, y_val, np.ones(len(X_train)), weights_val)\n",
    "y_pred_gb, rmse_gb = train_gradient_boost(X_train, y_train, X_val, y_val, np.ones(len(X_train)), weights_val)\n",
    "y_pred_gp, rmse_gp = train_gaussian_process(X_train, y_train, X_val, y_val, np.ones(len(X_train)), weights_val)\n",
    "y_pred_nn, rmse_nn = train_neural_network(X_train, y_train, X_val, y_val, np.ones(len(X_train)), weights_val)\n",
    "y_pred_c1, rmse_c1 = train_custom_model_1(X_train, y_train, X_val, y_val, np.ones(len(X_train)), weights_val)\n",
    "y_pred_c2, rmse_c2 = train_custom_model_2(X_train, y_train, X_val, y_val, np.ones(len(X_train)), weights_val)\n",
    "y_pred_tf, rmse_tf = train_template_fitting(X_train, y_train, X_val, y_val, np.ones(len(X_train)), weights_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
